{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 統計學習與深度學習 (Fall, 2024)\n",
    "### Homework 2\n",
    "\n",
    "請將IPYNB檔與IPYNB Export之HTML檔上傳至COOL作業區。作業自己做。嚴禁抄襲。不接受紙本繳交，不接受遲交。請以英文或中文作答。\n",
    "如無其他規定，所有重要結果應顯示至小數點第四位(四捨五入)。本次作業禁止使用Auto-SKlearn或其他AutoML工具。題目可能有額外實做限制。違反規定者該題以零分計算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一題[Logistic Regression: Probability, Loss, Gradient, and Weights]\n",
    "\n",
    "Logistic regression (LR) 是一個常用的分類模型。我們將在這個題目中透過練習熟悉一些LR的細節與特性。\n",
    "\n",
    "本題將利用UCI的\"Adult\" dataset <https://archive.ics.uci.edu/ml/datasets/Adult>來練習資料前處理。我們使用這個資料集的方式是用來建構預測最後一個收入欄位是'>50K'或'<=50K'。這個資料集已經先切好了Training跟Test。Training又切分為Subtraining與Validation兩個不重疊的集合。\n",
    "\n",
    "請直接使用作業附帶的資料檔 **adult_m50kv2.pickle**。讀取的方式為:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這是一個Dictionary結構，Keys有:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_train', 'y_train', 'x_test', 'y_test', 'columnname', 'num_col', 'x_subtrain', 'x_subvalid', 'y_subtrain', 'y_subvalid', 'x_subtrain_ib', 'y_subtrain_ib'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult50kp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中x_train與y_train為訓練資料，x_test與y_test為測試資料，columname為欄位名稱，\n",
    "x_subtrain與y_subtrain為Subtraining, x_subvalid與y_subvalid為validation資料集。\n",
    "\n",
    "本題的任務如下:\n",
    "\n",
    "**第一小題** (10%): 實做一個pred_prob函數。這個函數的輸入為資料矩陣、LR的常數項係數與特徵係數，輸出在給定係數下LR對各資料點預測屬於各Class的機率。本題為Binary Classification, 因此第一個Column為Class 0, 第二個Column為Class 1的機率。一般而言，Class 1稱為Postive Class, Class 0 稱為Negative Class。本題>50K為Positive Class。本題禁止直接使用sklearn中的實做。你應該使用Numpy建構此函數。然而，你可以參考sklearn中對此函數的定義<https://scikit-learn.org/1.5/modules/linear_model.html#binary-case>。你的實做應與此文件一致。\n",
    "\n",
    "請使用下面Python函數定義:\n",
    "\n",
    "```python\n",
    "def pred_prob(X, intercept, coefs, twocol = True):\n",
    "    # Implement your own probability function to predict \n",
    "    # the probability in binary logistic regression\n",
    "\n",
    "```\n",
    "\n",
    "其中`X`為特徵矩陣，每一個row為一筆資料，不包含常數項。\n",
    "`intercept`為LR的常數項係數。\n",
    "`coefs`為 K X 1 的係數Numpy向量。\n",
    "`twocol`為 Boolean。如True則回傳的機率矩陣為N X 2, 第一個Column為P(Y=0 | X) 第二個Column為P(Y=1 | X)。如False則回傳N X 1矩陣的P(Y=1 | X)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二小題** (15%): 實做LR with L2 Regularization的Loss Function。\n",
    "\n",
    "此Loss Function的定義如下\n",
    "\n",
    "$\\frac{1}{S} \\sum_{i=1}^{n} -s_i(y_i log(p(X_i) + (1 - y_i) log(1 - p(X_i)) + \\frac{1}{2} \\frac{w^T w}{S \\cdot  C}  $\n",
    "\n",
    "其中$P(X_i)$為LR在目前參數下預測資料點$i$為Positive Class的機率。\n",
    "$w$為參數向量(不包含常數項係數)。\n",
    "$s_i$為資料點$i$的全重，$s_i = 1$。\n",
    "$S = \\sum_{i=1}^N s_i$。\n",
    "$C$ 為Regularization Coefficient，數值越大則對係數的牽制越小(與課程投影片定義不同)。\n",
    "\n",
    "本題禁止直接使用sklearn中的實做。你應該使用Numpy建構此函數。然而，你可以參考sklearn中對此函數的定義<https://scikit-learn.org/1.5/modules/linear_model.html#binary-case>。你的實做應與此文件一致。\n",
    "\n",
    "請使用下面Python函數定義:\n",
    "\n",
    "```python\n",
    "def lr_logloss(Xtrain, ytrain, intercept, coefs, C):\n",
    "    # Implement your own loss function for\n",
    "    # Logistic Regression with L2 regularization\n",
    "    \n",
    "```\n",
    "\n",
    "其中`Xtrain`為特徵矩陣，每一個row為一筆資料，不包含常數項。\n",
    "`ytrain`為class label, 數值應為0或1。\n",
    "`intercept`為LR的常數項係數。\n",
    "`coefs`為 K X 1 的係數Numpy向量。\n",
    "`C`為regularization coefficient。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三小題** (10%): \n",
    "\n",
    "(1) 使用 sklearn.linear_model.LogisticRegression()與Sub-training 資料集學習LR參數，印出Intercept與各特徵名稱與係數。(2) 將學好的模型應用在Test Dataset，計算Accuracy, Recall, Precision, F1。可使用sklearn實做。務必在最後具體說明題目所要計算的數值。請勿只印出sklearn的output。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第四小題** (10%): Loss function visualization。\n",
    "\n",
    "基於前一小題學習出的係數，(1) 畫出在目前Intercept 附近一單位的Loss Function (2) 畫出在目前係數零(capital-loss)附近一單位的Loss Function。\n",
    "\n",
    "注意: 由於前一個小題學習出的係數已經對Loss Function最小化，你畫出的圖應該都是U形曲線。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "\n",
    "為了幫助同學們自我檢測正確性，在這裡提供幾個關鍵步驟的參考結果。\n",
    "\n",
    "**第一小題**\n",
    "```python\n",
    "# load dataset\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "X = adult50kp['x_subtrain'][0:5,]\n",
    "intercept = -1.5272275\n",
    "coefs = [0.25950781,  0.34876602,  2.31873776 , 0.78736064,  0.33992389,  0.08704992,\n",
    "  -0.43884149,  0.06617491, -0.86784172, -1.14138298, -0.0430126,   0.89943298,\n",
    "  -0.91920029,  0.11168262, -0.20330975, -0.45255335, -0.06209014, -1.1973518,\n",
    "  -0.32811911,  0.15581135,  0.23931349,  0.97771987, -0.80849467, -0.48881008,\n",
    "  -0.05761483, -0.61597391, -3.01467409,  0.67615709,  0.82684249,  0.45561501,\n",
    "   0.78515056,  0.10471469,  0.03536166, -0.18042056, -0.1981496,   0.89739856,\n",
    "   0.72133736,  0.19974049,  0.02431812, -0.54957554, -0.23797782, -0.19134163,\n",
    "  -0.08962951, -0.13136345, -0.14984098, -1.82034863,  0.22178759, -0.07676697,\n",
    "   1.5654472,   0.4801181,  -0.45519986, -2.16013255,  0.56715199, -1.36568413,\n",
    "  -0.26013768, -0.34160994,  0.30455591,  0.98946547,  0.83849021, -0.55428539,\n",
    "  -0.01931975,  0.02875685, -0.22012367,  0.165938,    0.24587743, -0.49584621,\n",
    "   0.27129184,  0.6632467,   1.08311314,  0.355352,    0.23657114, -0.55244221,\n",
    "  -0.29776791, -0.51492492, -1.08493494, -0.83233382,  0.61660701,  0.38077523,\n",
    "   0.34522743,  0.10493796,  0.16957889, -0.91882431, -0.07844291, -0.12138313,\n",
    "  -0.13627414,  0.437241,   -1.40284695,  0.43730002,  0.64656827, -0.12873123,\n",
    "   0.12363191,  0.30820418, -0.37598606, -0.2384826,  -1.86067539, -0.92314978,\n",
    "   2.17438484,  1.28232608, -1.05960696, -1.35676708, -0.99932736, -0.64333065]\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "coefs = coefs.reshape((-1, 1))\n",
    "pred_prob(X, intercept, coefs)\n",
    "```\n",
    "Output:\n",
    "```\n",
    "array([[0.85699649, 0.14300351],\n",
    "       [0.84396404, 0.15603596],\n",
    "       [0.54792834, 0.45207166],\n",
    "       [0.99604068, 0.00395932],\n",
    "       [0.91956486, 0.08043514]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二小題**\n",
    "\n",
    "```python\n",
    "Xtrain = adult50kp['x_subtrain']\n",
    "ytrain = adult50kp['y_subtrain']\n",
    "lr_logloss(Xtrain, ytrain, intercept, coefs, 1000)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "0.32275140356935755\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 第二題 [Chi-Squared Feature Selection] \n",
    "\n",
    "注意:\n",
    "* 本題應以Numpy實做，禁用現成的Chi-squared Feature Selection函數如sklearn.feature_selection.chi2()或scipy.stats.chisquare()或scipy.stats.chisquare()或scipy.stats.chi2_contingency()\n",
    "* sklearn.feature_selection.chi2() 實做的方式不是標準的Chi-squared test (cf. <https://github.com/scikit-learn/scikit-learn/blob/d5082d32d/sklearn/feature_selection/_univariate_selection.py#L195>)。本題要求你依照課程投影片的說明以標準的Chi-squared test方式實做。如果你的結果與 sklearn.feature_selection.chi2()是正常的。\n",
    "\n",
    "\n",
    "(30%) 在Univariate Feature Selection的情境下實做Chi-Squared Feature Selection Method。\n",
    "\n",
    "請使用下面Python函數定義:\n",
    "\n",
    "\n",
    "```python\n",
    "def my_chi2_fs(X, y):\n",
    "    # Compute chi-squared statistics for each columns in X\n",
    "    # X: pandas DataFrame; y: numpy array\n",
    "    # return: numpy array with chi-squared statistics  \n",
    "```\n",
    "\n",
    "為了方便批改與除錯，請將你的實做套用到mbav1.pickle的資料集中。資料集載入方式如下:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "dsfile = \"mbav1.pickle\"\n",
    "with open(dsfile, \"rb\") as fh1:\n",
    "    mba = pickle.load(fh1)\n",
    "\n",
    "catcol = mba['x_train'].select_dtypes('object').columns\n",
    "print(\"categorical columns:\", catcol)\n",
    "x_traincat = mba['x_train'][catcol].copy()\n",
    "```\n",
    "\n",
    "實做好的my_chi2_fs()應該有下面的行為:\n",
    "\n",
    "```python\n",
    "chi2vec = my_chi2_fs(x_traincat, mba['y_train'])\n",
    "for i, acol in enumerate(x_traincat.columns):\n",
    "    print(f\"{acol:20s} {chi2vec[i]:.4f}\")\n",
    "```\n",
    "\n",
    "Output\n",
    "\n",
    "```\n",
    "gender               0.1106\n",
    "major                4.1315\n",
    "race                 0.9505\n",
    "work_industry        16.3957\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三題 [Forward Feature Selection] \n",
    "\n",
    "注意: 本題禁用現成的Feature Selection Pipeline, 如sklearn.feature_selection.SelectFromModel()\n",
    "\n",
    "(25%) Forward Feature Selection是一個常見的Model-based Feature Selection Method。方法在概念上單純，但在實做上各異。我們在這裡練習一個使用Train-Valid-Test Split的Forward Feature Selection作法。這個作法使用Validation Set決定要把那個Feature加入。並在Validation Set的Performance不再進步時即停止整個程序。\n",
    "\n",
    "較詳細的作法如下:\n",
    "\n",
    "* Let 𝑀_0 denote the null model (no predictors); M_k = the model with k predictors (i.e., features); p = number of features\n",
    "* For Step k=0, 1, 2, …, 𝑝−1:\n",
    "    * Consider all p - k models that augment the predictors in 𝑀_k with one additional predictor. For each possibility; compute the prediction performance (F1 score) on the validation set and select one with the best performance.\n",
    "    * Stop if the best validation performance no longer increase.\n",
    "\n",
    "本題的資料集由pickle file載入 (A Dictionary):\n",
    "```python\n",
    "# load dataset\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "```\n",
    "\n",
    "* training set keys: x_subtrain, y_subtrain\n",
    "* validation set keys: x_subvalid, y_subvalid\n",
    "* test set keys: x_test, y_test\n",
    "\n",
    "為了方便比較，請使用下面的 LR Learner: sklearn.linear_model.LogisticRegression(solver = 'lbfgs', C= 1000, max_iter = 1000, tol=1e-5)\n",
    "\n",
    "(1) (5%) 總共有多少Features, 多少Training, Validation, Test Data?\n",
    "(2) (5%) 在Training Set上訓練，並在Validation Set與Test Set上計算F1 Score。\n",
    "(3) (15%) 使用上面的Forward Feature Selection作法，依序報告被選取的特徵與其Validation F1 Score與最後所有被選取的特徵的Test F1 Score。與(2)比較並討論。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sldl-pwhQZ0rV-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
