{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### çµ±è¨ˆå­¸ç¿’èˆ‡æ·±åº¦å­¸ç¿’ (Fall, 2024)\n",
    "### Homework 2\n",
    "\n",
    "è«‹å°‡IPYNBæª”èˆ‡IPYNB Exportä¹‹HTMLæª”ä¸Šå‚³è‡³COOLä½œæ¥­å€ã€‚ä½œæ¥­è‡ªå·±åšã€‚åš´ç¦æŠ„è¥²ã€‚ä¸æ¥å—ç´™æœ¬ç¹³äº¤ï¼Œä¸æ¥å—é²äº¤ã€‚è«‹ä»¥è‹±æ–‡æˆ–ä¸­æ–‡ä½œç­”ã€‚\n",
    "å¦‚ç„¡å…¶ä»–è¦å®šï¼Œæ‰€æœ‰é‡è¦çµæœæ‡‰é¡¯ç¤ºè‡³å°æ•¸é»ç¬¬å››ä½(å››æ¨äº”å…¥)ã€‚æœ¬æ¬¡ä½œæ¥­ç¦æ­¢ä½¿ç”¨Auto-SKlearnæˆ–å…¶ä»–AutoMLå·¥å…·ã€‚é¡Œç›®å¯èƒ½æœ‰é¡å¤–å¯¦åšé™åˆ¶ã€‚é•åè¦å®šè€…è©²é¡Œä»¥é›¶åˆ†è¨ˆç®—ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬ä¸€é¡Œ[Logistic Regression: Probability, Loss, Gradient, and Weights]\n",
    "\n",
    "Logistic regression (LR) æ˜¯ä¸€å€‹å¸¸ç”¨çš„åˆ†é¡æ¨¡å‹ã€‚æˆ‘å€‘å°‡åœ¨é€™å€‹é¡Œç›®ä¸­é€éç·´ç¿’ç†Ÿæ‚‰ä¸€äº›LRçš„ç´°ç¯€èˆ‡ç‰¹æ€§ã€‚\n",
    "\n",
    "æœ¬é¡Œå°‡åˆ©ç”¨UCIçš„\"Adult\" dataset <https://archive.ics.uci.edu/ml/datasets/Adult>ä¾†ç·´ç¿’è³‡æ–™å‰è™•ç†ã€‚æˆ‘å€‘ä½¿ç”¨é€™å€‹è³‡æ–™é›†çš„æ–¹å¼æ˜¯ç”¨ä¾†å»ºæ§‹é æ¸¬æœ€å¾Œä¸€å€‹æ”¶å…¥æ¬„ä½æ˜¯'>50K'æˆ–'<=50K'ã€‚é€™å€‹è³‡æ–™é›†å·²ç¶“å…ˆåˆ‡å¥½äº†Trainingè·ŸTestã€‚Trainingåˆåˆ‡åˆ†ç‚ºSubtrainingèˆ‡Validationå…©å€‹ä¸é‡ç–Šçš„é›†åˆã€‚\n",
    "\n",
    "è«‹ç›´æ¥ä½¿ç”¨ä½œæ¥­é™„å¸¶çš„è³‡æ–™æª” **adult_m50kv2.pickle**ã€‚è®€å–çš„æ–¹å¼ç‚º:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€™æ˜¯ä¸€å€‹Dictionaryçµæ§‹ï¼ŒKeysæœ‰:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x_train', 'y_train', 'x_test', 'y_test', 'columnname', 'num_col', 'x_subtrain', 'x_subvalid', 'y_subtrain', 'y_subvalid', 'x_subtrain_ib', 'y_subtrain_ib'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult50kp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¶ä¸­x_trainèˆ‡y_trainç‚ºè¨“ç·´è³‡æ–™ï¼Œx_testèˆ‡y_testç‚ºæ¸¬è©¦è³‡æ–™ï¼Œcolumnameç‚ºæ¬„ä½åç¨±ï¼Œ\n",
    "x_subtrainèˆ‡y_subtrainç‚ºSubtraining, x_subvalidèˆ‡y_subvalidç‚ºvalidationè³‡æ–™é›†ã€‚\n",
    "\n",
    "æœ¬é¡Œçš„ä»»å‹™å¦‚ä¸‹:\n",
    "\n",
    "**ç¬¬ä¸€å°é¡Œ** (10%): å¯¦åšä¸€å€‹pred_probå‡½æ•¸ã€‚é€™å€‹å‡½æ•¸çš„è¼¸å…¥ç‚ºè³‡æ–™çŸ©é™£ã€LRçš„å¸¸æ•¸é …ä¿‚æ•¸èˆ‡ç‰¹å¾µä¿‚æ•¸ï¼Œè¼¸å‡ºåœ¨çµ¦å®šä¿‚æ•¸ä¸‹LRå°å„è³‡æ–™é»é æ¸¬å±¬æ–¼å„Classçš„æ©Ÿç‡ã€‚æœ¬é¡Œç‚ºBinary Classification, å› æ­¤ç¬¬ä¸€å€‹Columnç‚ºClass 0, ç¬¬äºŒå€‹Columnç‚ºClass 1çš„æ©Ÿç‡ã€‚ä¸€èˆ¬è€Œè¨€ï¼ŒClass 1ç¨±ç‚ºPostive Class, Class 0 ç¨±ç‚ºNegative Classã€‚æœ¬é¡Œ>50Kç‚ºPositive Classã€‚æœ¬é¡Œç¦æ­¢ç›´æ¥ä½¿ç”¨sklearnä¸­çš„å¯¦åšã€‚ä½ æ‡‰è©²ä½¿ç”¨Numpyå»ºæ§‹æ­¤å‡½æ•¸ã€‚ç„¶è€Œï¼Œä½ å¯ä»¥åƒè€ƒsklearnä¸­å°æ­¤å‡½æ•¸çš„å®šç¾©<https://scikit-learn.org/1.5/modules/linear_model.html#binary-case>ã€‚ä½ çš„å¯¦åšæ‡‰èˆ‡æ­¤æ–‡ä»¶ä¸€è‡´ã€‚\n",
    "\n",
    "è«‹ä½¿ç”¨ä¸‹é¢Pythonå‡½æ•¸å®šç¾©:\n",
    "\n",
    "```python\n",
    "def pred_prob(X, intercept, coefs, twocol = True):\n",
    "    # Implement your own probability function to predict \n",
    "    # the probability in binary logistic regression\n",
    "\n",
    "```\n",
    "\n",
    "å…¶ä¸­`X`ç‚ºç‰¹å¾µçŸ©é™£ï¼Œæ¯ä¸€å€‹rowç‚ºä¸€ç­†è³‡æ–™ï¼Œä¸åŒ…å«å¸¸æ•¸é …ã€‚\n",
    "`intercept`ç‚ºLRçš„å¸¸æ•¸é …ä¿‚æ•¸ã€‚\n",
    "`coefs`ç‚º K X 1 çš„ä¿‚æ•¸Numpyå‘é‡ã€‚\n",
    "`twocol`ç‚º Booleanã€‚å¦‚Trueå‰‡å›å‚³çš„æ©Ÿç‡çŸ©é™£ç‚ºN X 2, ç¬¬ä¸€å€‹Columnç‚ºP(Y=0 | X) ç¬¬äºŒå€‹Columnç‚ºP(Y=1 | X)ã€‚å¦‚Falseå‰‡å›å‚³N X 1çŸ©é™£çš„P(Y=1 | X)ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¬¬äºŒå°é¡Œ** (15%): å¯¦åšLR with L2 Regularizationçš„Loss Functionã€‚\n",
    "\n",
    "æ­¤Loss Functionçš„å®šç¾©å¦‚ä¸‹\n",
    "\n",
    "$\\frac{1}{S} \\sum_{i=1}^{n} -s_i(y_i log(p(X_i) + (1 - y_i) log(1 - p(X_i)) + \\frac{1}{2} \\frac{w^T w}{S \\cdot  C}  $\n",
    "\n",
    "å…¶ä¸­$P(X_i)$ç‚ºLRåœ¨ç›®å‰åƒæ•¸ä¸‹é æ¸¬è³‡æ–™é»$i$ç‚ºPositive Classçš„æ©Ÿç‡ã€‚\n",
    "$w$ç‚ºåƒæ•¸å‘é‡(ä¸åŒ…å«å¸¸æ•¸é …ä¿‚æ•¸)ã€‚\n",
    "$s_i$ç‚ºè³‡æ–™é»$i$çš„å…¨é‡ï¼Œ$s_i = 1$ã€‚\n",
    "$S = \\sum_{i=1}^N s_i$ã€‚\n",
    "$C$ ç‚ºRegularization Coefficientï¼Œæ•¸å€¼è¶Šå¤§å‰‡å°ä¿‚æ•¸çš„ç‰½åˆ¶è¶Šå°(èˆ‡èª²ç¨‹æŠ•å½±ç‰‡å®šç¾©ä¸åŒ)ã€‚\n",
    "\n",
    "æœ¬é¡Œç¦æ­¢ç›´æ¥ä½¿ç”¨sklearnä¸­çš„å¯¦åšã€‚ä½ æ‡‰è©²ä½¿ç”¨Numpyå»ºæ§‹æ­¤å‡½æ•¸ã€‚ç„¶è€Œï¼Œä½ å¯ä»¥åƒè€ƒsklearnä¸­å°æ­¤å‡½æ•¸çš„å®šç¾©<https://scikit-learn.org/1.5/modules/linear_model.html#binary-case>ã€‚ä½ çš„å¯¦åšæ‡‰èˆ‡æ­¤æ–‡ä»¶ä¸€è‡´ã€‚\n",
    "\n",
    "è«‹ä½¿ç”¨ä¸‹é¢Pythonå‡½æ•¸å®šç¾©:\n",
    "\n",
    "```python\n",
    "def lr_logloss(Xtrain, ytrain, intercept, coefs, C):\n",
    "    # Implement your own loss function for\n",
    "    # Logistic Regression with L2 regularization\n",
    "    \n",
    "```\n",
    "\n",
    "å…¶ä¸­`Xtrain`ç‚ºç‰¹å¾µçŸ©é™£ï¼Œæ¯ä¸€å€‹rowç‚ºä¸€ç­†è³‡æ–™ï¼Œä¸åŒ…å«å¸¸æ•¸é …ã€‚\n",
    "`ytrain`ç‚ºclass label, æ•¸å€¼æ‡‰ç‚º0æˆ–1ã€‚\n",
    "`intercept`ç‚ºLRçš„å¸¸æ•¸é …ä¿‚æ•¸ã€‚\n",
    "`coefs`ç‚º K X 1 çš„ä¿‚æ•¸Numpyå‘é‡ã€‚\n",
    "`C`ç‚ºregularization coefficientã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¬¬ä¸‰å°é¡Œ** (10%): \n",
    "\n",
    "(1) ä½¿ç”¨ sklearn.linear_model.LogisticRegression()èˆ‡Sub-training è³‡æ–™é›†å­¸ç¿’LRåƒæ•¸ï¼Œå°å‡ºInterceptèˆ‡å„ç‰¹å¾µåç¨±èˆ‡ä¿‚æ•¸ã€‚(2) å°‡å­¸å¥½çš„æ¨¡å‹æ‡‰ç”¨åœ¨Test Datasetï¼Œè¨ˆç®—Accuracy, Recall, Precision, F1ã€‚å¯ä½¿ç”¨sklearnå¯¦åšã€‚å‹™å¿…åœ¨æœ€å¾Œå…·é«”èªªæ˜é¡Œç›®æ‰€è¦è¨ˆç®—çš„æ•¸å€¼ã€‚è«‹å‹¿åªå°å‡ºsklearnçš„outputã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¬¬å››å°é¡Œ** (10%): Loss function visualizationã€‚\n",
    "\n",
    "åŸºæ–¼å‰ä¸€å°é¡Œå­¸ç¿’å‡ºçš„ä¿‚æ•¸ï¼Œ(1) ç•«å‡ºåœ¨ç›®å‰Intercept é™„è¿‘ä¸€å–®ä½çš„Loss Function (2) ç•«å‡ºåœ¨ç›®å‰ä¿‚æ•¸é›¶(capital-loss)é™„è¿‘ä¸€å–®ä½çš„Loss Functionã€‚\n",
    "\n",
    "æ³¨æ„: ç”±æ–¼å‰ä¸€å€‹å°é¡Œå­¸ç¿’å‡ºçš„ä¿‚æ•¸å·²ç¶“å°Loss Functionæœ€å°åŒ–ï¼Œä½ ç•«å‡ºçš„åœ–æ‡‰è©²éƒ½æ˜¯Uå½¢æ›²ç·šã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check\n",
    "\n",
    "ç‚ºäº†å¹«åŠ©åŒå­¸å€‘è‡ªæˆ‘æª¢æ¸¬æ­£ç¢ºæ€§ï¼Œåœ¨é€™è£¡æä¾›å¹¾å€‹é—œéµæ­¥é©Ÿçš„åƒè€ƒçµæœã€‚\n",
    "\n",
    "**ç¬¬ä¸€å°é¡Œ**\n",
    "```python\n",
    "# load dataset\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "X = adult50kp['x_subtrain'][0:5,]\n",
    "intercept = -1.5272275\n",
    "coefs = [0.25950781,  0.34876602,  2.31873776 , 0.78736064,  0.33992389,  0.08704992,\n",
    "  -0.43884149,  0.06617491, -0.86784172, -1.14138298, -0.0430126,   0.89943298,\n",
    "  -0.91920029,  0.11168262, -0.20330975, -0.45255335, -0.06209014, -1.1973518,\n",
    "  -0.32811911,  0.15581135,  0.23931349,  0.97771987, -0.80849467, -0.48881008,\n",
    "  -0.05761483, -0.61597391, -3.01467409,  0.67615709,  0.82684249,  0.45561501,\n",
    "   0.78515056,  0.10471469,  0.03536166, -0.18042056, -0.1981496,   0.89739856,\n",
    "   0.72133736,  0.19974049,  0.02431812, -0.54957554, -0.23797782, -0.19134163,\n",
    "  -0.08962951, -0.13136345, -0.14984098, -1.82034863,  0.22178759, -0.07676697,\n",
    "   1.5654472,   0.4801181,  -0.45519986, -2.16013255,  0.56715199, -1.36568413,\n",
    "  -0.26013768, -0.34160994,  0.30455591,  0.98946547,  0.83849021, -0.55428539,\n",
    "  -0.01931975,  0.02875685, -0.22012367,  0.165938,    0.24587743, -0.49584621,\n",
    "   0.27129184,  0.6632467,   1.08311314,  0.355352,    0.23657114, -0.55244221,\n",
    "  -0.29776791, -0.51492492, -1.08493494, -0.83233382,  0.61660701,  0.38077523,\n",
    "   0.34522743,  0.10493796,  0.16957889, -0.91882431, -0.07844291, -0.12138313,\n",
    "  -0.13627414,  0.437241,   -1.40284695,  0.43730002,  0.64656827, -0.12873123,\n",
    "   0.12363191,  0.30820418, -0.37598606, -0.2384826,  -1.86067539, -0.92314978,\n",
    "   2.17438484,  1.28232608, -1.05960696, -1.35676708, -0.99932736, -0.64333065]\n",
    "\n",
    "coefs = np.array(coefs)\n",
    "coefs = coefs.reshape((-1, 1))\n",
    "pred_prob(X, intercept, coefs)\n",
    "```\n",
    "Output:\n",
    "```\n",
    "array([[0.85699649, 0.14300351],\n",
    "       [0.84396404, 0.15603596],\n",
    "       [0.54792834, 0.45207166],\n",
    "       [0.99604068, 0.00395932],\n",
    "       [0.91956486, 0.08043514]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ç¬¬äºŒå°é¡Œ**\n",
    "\n",
    "```python\n",
    "Xtrain = adult50kp['x_subtrain']\n",
    "ytrain = adult50kp['y_subtrain']\n",
    "lr_logloss(Xtrain, ytrain, intercept, coefs, 1000)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "0.32275140356935755\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ç¬¬äºŒé¡Œ [Chi-Squared Feature Selection] \n",
    "\n",
    "æ³¨æ„:\n",
    "* æœ¬é¡Œæ‡‰ä»¥Numpyå¯¦åšï¼Œç¦ç”¨ç¾æˆçš„Chi-squared Feature Selectionå‡½æ•¸å¦‚sklearn.feature_selection.chi2()æˆ–scipy.stats.chisquare()æˆ–scipy.stats.chisquare()æˆ–scipy.stats.chi2_contingency()\n",
    "* sklearn.feature_selection.chi2() å¯¦åšçš„æ–¹å¼ä¸æ˜¯æ¨™æº–çš„Chi-squared test (cf. <https://github.com/scikit-learn/scikit-learn/blob/d5082d32d/sklearn/feature_selection/_univariate_selection.py#L195>)ã€‚æœ¬é¡Œè¦æ±‚ä½ ä¾ç…§èª²ç¨‹æŠ•å½±ç‰‡çš„èªªæ˜ä»¥æ¨™æº–çš„Chi-squared testæ–¹å¼å¯¦åšã€‚å¦‚æœä½ çš„çµæœèˆ‡ sklearn.feature_selection.chi2()æ˜¯æ­£å¸¸çš„ã€‚\n",
    "\n",
    "\n",
    "(30%) åœ¨Univariate Feature Selectionçš„æƒ…å¢ƒä¸‹å¯¦åšChi-Squared Feature Selection Methodã€‚\n",
    "\n",
    "è«‹ä½¿ç”¨ä¸‹é¢Pythonå‡½æ•¸å®šç¾©:\n",
    "\n",
    "\n",
    "```python\n",
    "def my_chi2_fs(X, y):\n",
    "    # Compute chi-squared statistics for each columns in X\n",
    "    # X: pandas DataFrame; y: numpy array\n",
    "    # return: numpy array with chi-squared statistics  \n",
    "```\n",
    "\n",
    "ç‚ºäº†æ–¹ä¾¿æ‰¹æ”¹èˆ‡é™¤éŒ¯ï¼Œè«‹å°‡ä½ çš„å¯¦åšå¥—ç”¨åˆ°mbav1.pickleçš„è³‡æ–™é›†ä¸­ã€‚è³‡æ–™é›†è¼‰å…¥æ–¹å¼å¦‚ä¸‹:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "dsfile = \"mbav1.pickle\"\n",
    "with open(dsfile, \"rb\") as fh1:\n",
    "    mba = pickle.load(fh1)\n",
    "\n",
    "catcol = mba['x_train'].select_dtypes('object').columns\n",
    "print(\"categorical columns:\", catcol)\n",
    "x_traincat = mba['x_train'][catcol].copy()\n",
    "```\n",
    "\n",
    "å¯¦åšå¥½çš„my_chi2_fs()æ‡‰è©²æœ‰ä¸‹é¢çš„è¡Œç‚º:\n",
    "\n",
    "```python\n",
    "chi2vec = my_chi2_fs(x_traincat, mba['y_train'])\n",
    "for i, acol in enumerate(x_traincat.columns):\n",
    "    print(f\"{acol:20s} {chi2vec[i]:.4f}\")\n",
    "```\n",
    "\n",
    "Output\n",
    "\n",
    "```\n",
    "gender               0.1106\n",
    "major                4.1315\n",
    "race                 0.9505\n",
    "work_industry        16.3957\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¬¬ä¸‰é¡Œ [Forward Feature Selection] \n",
    "\n",
    "æ³¨æ„: æœ¬é¡Œç¦ç”¨ç¾æˆçš„Feature Selection Pipeline, å¦‚sklearn.feature_selection.SelectFromModel()\n",
    "\n",
    "(25%) Forward Feature Selectionæ˜¯ä¸€å€‹å¸¸è¦‹çš„Model-based Feature Selection Methodã€‚æ–¹æ³•åœ¨æ¦‚å¿µä¸Šå–®ç´”ï¼Œä½†åœ¨å¯¦åšä¸Šå„ç•°ã€‚æˆ‘å€‘åœ¨é€™è£¡ç·´ç¿’ä¸€å€‹ä½¿ç”¨Train-Valid-Test Splitçš„Forward Feature Selectionä½œæ³•ã€‚é€™å€‹ä½œæ³•ä½¿ç”¨Validation Setæ±ºå®šè¦æŠŠé‚£å€‹FeatureåŠ å…¥ã€‚ä¸¦åœ¨Validation Setçš„Performanceä¸å†é€²æ­¥æ™‚å³åœæ­¢æ•´å€‹ç¨‹åºã€‚\n",
    "\n",
    "è¼ƒè©³ç´°çš„ä½œæ³•å¦‚ä¸‹:\n",
    "\n",
    "* Let ğ‘€_0 denote the null model (no predictors); M_k = the model with k predictors (i.e., features); p = number of features\n",
    "* For Step k=0, 1, 2, â€¦, ğ‘âˆ’1:\n",
    "    * Consider all p - k models that augment the predictors in ğ‘€_k with one additional predictor. For each possibility; compute the prediction performance (F1 score) on the validation set and select one with the best performance.\n",
    "    * Stop if the best validation performance no longer increase.\n",
    "\n",
    "æœ¬é¡Œçš„è³‡æ–™é›†ç”±pickle fileè¼‰å…¥ (A Dictionary):\n",
    "```python\n",
    "# load dataset\n",
    "dsfile = 'adult_m50kv2.pickle'\n",
    "with open(dsfile, 'rb') as fh1:\n",
    "    adult50kp = pickle.load(fh1)\n",
    "\n",
    "```\n",
    "\n",
    "* training set keys: x_subtrain, y_subtrain\n",
    "* validation set keys: x_subvalid, y_subvalid\n",
    "* test set keys: x_test, y_test\n",
    "\n",
    "ç‚ºäº†æ–¹ä¾¿æ¯”è¼ƒï¼Œè«‹ä½¿ç”¨ä¸‹é¢çš„ LR Learner: sklearn.linear_model.LogisticRegression(solver = 'lbfgs', C= 1000, max_iter = 1000, tol=1e-5)\n",
    "\n",
    "(1) (5%) ç¸½å…±æœ‰å¤šå°‘Features, å¤šå°‘Training, Validation, Test Data?\n",
    "(2) (5%) åœ¨Training Setä¸Šè¨“ç·´ï¼Œä¸¦åœ¨Validation Setèˆ‡Test Setä¸Šè¨ˆç®—F1 Scoreã€‚\n",
    "(3) (15%) ä½¿ç”¨ä¸Šé¢çš„Forward Feature Selectionä½œæ³•ï¼Œä¾åºå ±å‘Šè¢«é¸å–çš„ç‰¹å¾µèˆ‡å…¶Validation F1 Scoreèˆ‡æœ€å¾Œæ‰€æœ‰è¢«é¸å–çš„ç‰¹å¾µçš„Test F1 Scoreã€‚èˆ‡(2)æ¯”è¼ƒä¸¦è¨è«–ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sldl-pwhQZ0rV-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
